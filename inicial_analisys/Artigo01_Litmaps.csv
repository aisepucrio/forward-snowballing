DOI,Title,Authors,Journal,Year,Abstract,LitmapsId,Cited By,References,PubMedId,Tags
10.1145/3579028.3609016,Generative AI for Reengineering Variants into Software Product Lines: An Experience Report,"M. Acher, Jabier Martinez",Software Product Lines Conference,2023,"The migration and reengineering of existing variants into a software product line (SPL) is an error-prone and time-consuming activity. Many extractive approaches have been proposed, spanning different activities from feature identification and naming to the synthesis of reusable artefacts. In this paper, we explore how large language model (LLM)-based assistants can support domain analysts and developers. We revisit four illustrative cases of the literature where the challenge is to migrate variants written in different formalism (UML class diagrams, Java, GraphML, statecharts). We systematically report on our experience with ChatGPT-4, describing our strategy to prompt LLMs and documenting positive aspects but also failures. We compare the use of LLMs with state-of-the-art approach, BUT4Reuse. While LLMs offer potential in assisting domain analysts and developers in transitioning software variants into SPLs, their intrinsic stochastic nature and restricted ability to manage large variants or complex structures necessitate a semiautomatic approach, complete with careful review, to counteract inaccuracies.",265806909,8,66,,
10.1145/3377024.3380451,Next steps in variability management due to autonomous behaviour and runtime learning,N. Bencomo,VaMoS,2020,"One of the basic principles in product lines is to delay design decisions related to offered functionality and quality to later phases of the life cycle [25]. Instead of deciding on what system to develop in advance, a set of assets and a common reference architecture are specified and implemented during the Domain Engineering process. Later on, during Application Engineering, specific systems are developed to satisfy the requirements reusing the assets and architecture [16]. Traditionally, this is during the Application Engineering when delayed design decisions are solved. The realization of this delay relies heavily on the use of variability in the development of product lines and systems. However, as systems become more interconnected and diverse, software architects cannot easily foresee the software variants and the interconnections between components. Consequently, a generic a priori model is conceived to specify the system's dynamic behaviour and architecture. The corresponding design decisions are left to be solved at runtime [13]. Surprisingly, few research initiatives have investigated variability models at runtime [9]. Further, they have been applied only at the level of goals and architecture, which contrasts to the needs claimed by the variability community, i.e., Software Product Lines (SPLC) and Dynamic Software Product Lines (DSPL) [2, 10, 14, 22]. Especially, the vision of DSPL with their ability to support runtime updates with virtually zero downtime for products of a software product line, denotes the obvious need of variability models being used at runtime to adapt the corresponding programs. A main challenge for dealing with runtime variability is that it should support a wide range of product customizations under various scenarios that might be unknown until the execution time, as new product variants can be identified only at runtime [10, 11]. Contemporary variability models face the challenge of representing runtime variability to therefore allow the modification of variation points during the system's execution, and underpin the automation of the system's reconfiguration [15]. The runtime representation of feature models (i.e. the runtime model of features) is required to automate the decision making [9]. Software automation and adaptation techniques have traditionally required a priori models for the dynamic behaviour of systems [17]. With the uncertainty present in the scenarios involved, the a priori model is difficult to define [20, 23, 26]. Even if foreseen, its maintenance is labour-intensive and, due to architecture decay, it is also prone to get out-of-date. However, the use of models@runtime does not necessarily require defining the system's behaviour model beforehand. Instead, different techniques such as machine learning, or mining software component interactions from system execution traces can be used to build a model which is in turn used to analyze, plan, and execute adaptations [18], and synthesize emergent software on the fly [7]. Another well-known problem posed by the uncertainty that characterize autonomous systems is that different stakeholders (e.g. end users, operators and even developers) may not understand them due to the emergent behaviour. In other words, the running system may surprise its customers and/or developers [4]. The lack of support for explanation in these cases may compromise the trust to stakeholders, who may eventually stop using a system [12, 24]. I speculate that variability models can offer great support for (i) explanation to understand the diversity of the causes and triggers of decisions during execution and their corresponding effects using traceability [5], and (ii) better understand the behaviour of the system and its environment. Further, an extension and potentially reframing of the techniques associated with variability management may be needed to help taming uncertainty and support explanation and understanding of the systems. The use of new techniques such as machine learning exacerbates the current situation. However, at the same time machine learning techniques can also help and be used, for example, to explore the variability space [1]. What can the community do to face the challenges associated? We need to meaningfully incorporate techniques from areas such as artificial intelligence, machine learning, optimization, planning, decision theory, and bio-inspired computing into our variability management techniques to provide explanation and management of the diversity of decisions, their causes and the effects associated. My own previous work has progressed [3, 5, 6, 8, 11, 12, 19, 21] to reflect what was discussed above.",124159861,0,24,,
10.1145/3336294.3342383,Machine Learning and Configurable Systems: A Gentle Introduction,"Hugo Martin, Juliana Alves Pereira, M. Acher, Paul Temple",SPLC,2019,"The goal of this tutorial is to give a gentle introduction to how machine learning can be used to support software product line configuration. This is our second practical tutorial in this trending field. The tutorial is based on a systematic literature review and includes practical tasks (specialization, performance and bug prediction) on real-world systems (Linux, VaryLaTeX, x264). The material is designed for academics and practitioners with basic knowledge in software product lines and machine learning.",106842904,6,5,,
10.1145/3503229.3547057,Defining categorical reasoning of numerical feature models with feature-wise and variant-wise quality attributes,"Daniel-Jesus Munoz, M. Pinto, D. Gurov, L. Fuentes",SPLC,2022,"Automatic analysis of variability is an important stage of Software Product Line (SPL) engineering. Incorporating quality information into this stage poses a significant challenge. However, quality-aware automated analysis tools are rare, mainly because in existing solutions variability and quality information are not unified under the same model. In this paper, we make use of the Quality Variability Model (QVM), based on Category Theory (CT), to redefine reasoning operations. We start defining and composing the six most common operations in SPL, but now as quality-based queries, which tend to be unavailable in other approaches. Consequently, QVM supports interactions between variant-wise and feature-wise quality attributes. As a proof of concept, we present, implement and execute the operations as lambda reasoning for CQL IDE - the state-of-the-art CT tool.",245589381,0,38,,
10.11591/ijeecs.v32.i3.pp1774-1784,Global software development agile planning model: challenges and current trends,"Hajar Lamsellak, Mohammed Ghaouth Belkasmi",Indonesian Journal of Electrical Engineering and Computer Science,2023,"Agile planning offers a number of benefits that make the customers active members of the team throughout the project. In global software development (GSD), geographic separations demand special attention to harness these benefits. Our paper conducted a systematic mapping study (SMS) to analyze GSD-specific agile planning challenges followed by a systematic literature review (SLR) for efficient solutions. These studies led to a model for agile planning in global software development supporting GSD practitioners during this process.",266337499,1,120,,
10.1145/3503229.3547026,kconfig-webconf: retrofitting performance models onto kconfig-based software product lines,"Daniel Friesel, Kathrin Elmenhorst, L. Kaiser, Michael MÃ¼ller, O. Spinczyk",Software Product Lines Conference,2022,"Despite decades of research and clear advantages, performance-aware configuration of real-world software product lines is still an exception rather than the norm. One reason for this may be tooling: configuration software with support for non-functional property models is generally not compatible with the configuration and build process of existing product lines. Specifically, the Kconfig language is popular in open source software projects, but neither language nor configuration frontends support performance models. To address this, we present kconfig-webconf: a performance-aware, Kconfig-compatible software product line configuration frontend. It is part of a toolchain that can automatically generate performance models with a minimal amount of changes to a software product line's build process. With such a performance model, kconfig-webconf can serve as a performance-aware drop-in replacement for existing Kconfig frontends. We evaluate its usage in five examples, including the busybox multi-call binary and the resKIL agricultural AI product line.",237789890,1,13,,
10.1145/3702986,Deep Configuration Performance Learning: A Systematic Survey and Taxonomy,"Jingzhi Gong, Tao Chen",ACM Transactions on Software Engineering and Methodology,2024,"Performance is arguably the most crucial attribute that reflects the quality of a configurable software system. However, given the increasing scale and complexity of modern software, modeling and predicting how various configurations can impact performance becomes one of the major challenges in software maintenance. As such, performance is often modeled without having a thorough knowledge of the software system, but relying mainly on data, which fits precisely with the purpose of deep learning. In this paper, we conduct a comprehensive review exclusively on the topic of deep learning for performance learning of configurable software, covering 1,206 searched papers spanning six indexing services, based on which 99 primary papers were extracted and analyzed. Our results outline key statistics, taxonomy, strengths, weaknesses, and optimal usage scenarios for techniques related to the preparation of configuration data, the construction of deep learning performance models, the evaluation of these models, and their utilization in various software configuration-related tasks. We also identify the good practices and potentially problematic phenomena from the studies surveyed, together with a comprehensive summary of actionable suggestions and insights into future opportunities within the field. To promote open science, all the raw results of this survey can be accessed at our repository: https://github.com/ideas-labo/DCPL-SLR .",280837453,1,94,,
10.1145/3634713.3634732,A Demonstration of End-User Code Customization Using Generative AI,Mathieu Acher,International Workshop on Variability Modelling of Software-Intensive Systems,2024,"Producing a variant of code is highly challenging, particularly for individuals unfamiliar with programming. This demonstration introduces a novel use of generative AI to aid end-users in customizing code. We first describe how generative AI can be used to customize code through prompts and instructions, and further demonstrate its potential in building end-user tools for configuring code. We showcase how to transform an undocumented, technical, low-level TikZ into a user-friendly, configurable, Web-based customization tool written in Python, HTML, CSS, and JavaScript and itself configurable. We discuss how generative AI can support this transformation process and traditional variability engineering tasks, such as identification and implementation of features, synthesis of a template code generator, and development of end-user configura-tors. We believe it is a first step towards democratizing variability programming, opening a path for end-users to adapt code to their needs.",269051327,1,37,,
10.48550/arxiv.2301.07690,CaRE: Finding Root Causes of Configuration Issues in Highly-Configurable Robots,"Md. Abir Hossen, Sonam Kharade, B. Schmerl, Javier C'amara, Jason O'Kane, E. Czaplinski, Katherine A. Dzurilla, D. Garlan, Pooyan Jamshidi",IEEE Robotics and Automation Letters,2023,"Robotic systems have subsystems with a combinatorially large configuration space and hundreds or thousands of possible software and hardware configuration options interacting non-trivially. The configurable parameters are set to target specific objectives, but they can cause functional faults when incorrectly configured. Finding the root cause of such faults is challenging due to the exponentially large configuration space and the dependencies between the robot's configuration settings and performance. This paper proposes C<sc>a</sc>REâa method for diagnosing the root cause of functional faults through the lens of causality. C<sc>a</sc>RE abstracts the causal relationships between various configuration options and the robot's performance objectives by learning a causal structure and estimating the causal effects of options on robot performance indicators. We demonstrate C<sc>a</sc>REâs efficacy by finding the root cause of the observed functional faults and validating the diagnosed root cause by conducting experiments in both physical robots (<italic>Husky</italic> and <italic>Turtlebot 3</italic>) and in simulation (<italic>Gazebo</italic>). Furthermore, we demonstrate that the causal models learned from robots in simulation (e.g., <italic>Husky</italic> in <italic>Gazebo</italic>) are transferable to physical robots across different platforms (e.g., <italic>Husky</italic> and <italic>Turtlebot 3</italic>).",252734186,7,61,,
10.1145/3579027.3608972,On Programming Variability with Large Language Model-based Assistant,"M. Acher, J. Duarte, J. JÃ©zÃ©quel",Software Product Lines Conference,2023,"Programming variability is central to the design and implementation of software systems that can adapt to a variety of contexts and requirements, providing increased flexibility and customization. Managing the complexity that arises from having multiple features, variations, and possible configurations is known to be highly challenging for software developers. In this paper, we explore how large language model (LLM)-based assistants can support the programming of variability.We report on new approaches made possible with LLM-based assistants, like: features and variations can be implemented as prompts; augmentation of variability out of LLM-based domain knowledge; seamless implementation of variability in different kinds of artefacts, programming languages, and frameworks, at different binding times (compile-time or run-time). We are sharing our data (prompts, sessions, generated code, etc.) to support the assessment of the effectiveness and robustness of LLMs for variability-related tasks.",263828907,9,60,,
10.1109/cps-iotbench56135.2022.00007,Regression Model Trees: Compact Energy Models for Complex IoT Devices,"Daniel Friesel, O. Spinczyk",2022 Workshop on Benchmarking Cyber-Physical Systems and Internet of Things (CPS-IoTBench),2022,"The energy and timing behaviour of embedded components such as radio chips or sensors plays an important role when developing energy-efficient cyber-physical systems and IoT devices. However, datasheet values generally have low accuracy and may be incomplete, and performing new energy measurements after each code or hardware configuration change is time-consuming. While energy models â automatically generated from benchmarks exercising all relevant device configurations â offer a solution, they should have both low prediction error and low complexity in order to be useful to humans as well as energy simulations. With todayâs increasingly complex devices and drivers, generating compact and accurate energy models is becoming harder due to non-linear effects and interdependencies between configuration parameters. To address this issue, we present Regression Model Trees. By combining software product line engineering and energy modeling methodologies, these are capable of automatically learning complex energy models from benchmark data. Using energy and timing benchmarks on two embedded radio chips and an air quality sensor, we show that Regression Model Trees are both more accurate than conventional energy models and less complex than state-of-the-art approaches from the product line engineering community. Thus, they are easier to understand and use for humans and algorithms alike. We observe two-to 100-fold complexity reduction, and a maximum energy model error of 6 % with cross-validation.",55787848,5,24,,
10.1145/3461001.3471155,A comparison of performance specialization learning for configurable systems,"Hugo Martin, M. Acher, Juliana Alves Pereira, J. JÃ©zÃ©quel",Software Product Lines Conference,2021,"The specialization of the configuration space of a software system has been considered for targeting specific configuration profiles, usages, deployment scenarios, or hardware settings. The challenge is to find constraints among options' values that only retain configurations meeting a performance objective. Since the exponential nature of configurable systems makes a manual specialization unpractical, several approaches have considered its automation using machine learning, i.e., measuring a sample of configurations and then learning what options' values should be constrained. Even focusing on learning techniques based on decision trees for their built-in explainability, there is still a wide range of possible approaches that need to be evaluated, i.e., how accurate is the specialization with regards to sampling size, performance thresholds, and kinds of configurable systems. In this paper, we compare six learning techniques: three variants of decision trees (including a novel algorithm) with and without the use of model-based feature selection. We first perform a study on 8 configurable systems considered in previous related works and show that the accuracy reaches more than 90% and that feature selection can improve the results in the majority of cases. We then perform a study on the Linux kernel and show that these techniques performs as well as on the other systems. Overall, our results show that there is no one-size-fits-all learning variant (though high accuracy can be achieved): we present guidelines and discuss tradeoffs.",139926678,12,47,,
10.48550/arxiv.2204.12918,We're not gonna break it!: consistency-preserving operators for efficient product line configuration,"J. Horcas, D. StrÃ¼ber, Alexandru Burdusel, Jabier Martinez, S. Zschaler",SPLC,2022,"When configuring a software product line, finding a good trade-off between multiple orthogonal quality concerns is a challenging multi-objective optimisation problem. State-of-the-art solutions based on search-based techniques create invalid configurations in intermediate steps, requiring additional repair actions that reduce the efficiency of the search. In this work, we introduce consistency-preserving configuration operators (CPCOs)âgenetic operators that maintain valid configurations throughout the entire search. CPCOs bundle coherent sets of changes: the activation or deactivation of a particular feature together with other (de)activations that are needed to preserve validity. In our evaluation, our instantiation of the IBEA algorithm with CPCOs outperforms two state-of-the-art tools for optimal product line configuration in terms of both speed and solution quality. The improvements are especially pronounced in large product lines with thousands of features.",262629201,11,63,,
(missing DOI),Monte Carlo Simulations for Variability Analyses in Highly Configurable Systems,"JosÃ© Miguel Horcas Aguilera, A. G. MÃ¡rquez, J. Galindo, David Benavides",ConfWS,2021,". Highly conï¬gurable systems expose numerous variation points to be conï¬gured by the stakeholders. Deciding which variant to select for a given variation point is hard to know a priori because each variant affects the conï¬guration properties ( e.g., performance, efï¬ciency, fault tolerance) differently, and evaluating all conï¬gurations is practically infeasible. This paper introduces an approach based on Monte Carlo simulations to analyze the inï¬uence of each feature selection when conï¬guring a variability model. We split the whole conï¬guration space into step-wise decisions driven by the variation points that a user normally needs to face/decide during the conï¬guration process. Monte Carlo simulations approximate the in-ï¬uence of each feature variant evaluating as few conï¬gurations as possible. Our solution complements existing sampling techniques to analyze colossal conï¬guration spaces improving the understanding of the inï¬uence of each feature selection. It can be part of a decision-making tool to assist the user by means of recommendation systems and interactive conï¬guration processes.",233855750,1,45,,
10.1109/tse.2025.3525955,Accuracy Can Lie: On the Impact of Surrogate Model in Configuration Tuning,"Pengzhou Chen, Jingzhi Gong, Tao Chen",arXiv (Cornell University),2025,"To ease the expensive measurements during configuration tuning, it is natural to build a surrogate model as the replacement of the system, and thereby the configuration performance can be cheaply evaluated. Yet, a stereotype therein is that the higher the model accuracy, the better the tuning result would be. This""accuracy is all""belief drives our research community to build more and more accurate models and criticize a tuner for the inaccuracy of the model used. However, this practice raises some previously unaddressed questions, e.g., Do those somewhat small accuracy improvements reported in existing work really matter much to the tuners? What role does model accuracy play in the impact of tuning quality? To answer those related questions, we conduct one of the largest-scale empirical studies to date-running over the period of 13 months 24*7-that covers 10 models, 17 tuners, and 29 systems from the existing works while under four different commonly used metrics, leading to 13,612 cases of investigation. Surprisingly, our key findings reveal that the accuracy can lie: there are a considerable number of cases where higher accuracy actually leads to no improvement in the tuning outcomes (up to 58% cases under certain setting), or even worse, it can degrade the tuning quality (up to 24% cases under certain setting). We also discover that the chosen models in most proposed tuners are sub-optimal and that the required % of accuracy change to significantly improve tuning quality varies according to the range of model accuracy. Deriving from the fitness landscape analysis, we provide in-depth discussions of the rationale behind, offering several lessons learned as well as insights for future opportunities. Most importantly, this work poses a clear message to the community: we should take one step back from the natural""accuracy is all""belief for model-based configuration tuning.",282924515,1,88,,
10.1145/3503229.3547026,kconfig-webconf,"Daniel Friesel, Kathrin Elmenhorst, Lennart Kaiser, Michael MÃ¼ller, Olaf Spinczyk, Daniel Friesel, Kathrin Elmenhorst, Lennart Kaiser, Michael MÃ¼ller, Olaf Spinczyk",(missing journal),2022,(missing abstract),251854615,1,9,,
(missing DOI),Solving Multi-Configuration Problems: A Performance Analysis with Choco Solver,"Benjamin Ritz, A. Felfernig, Viet-Man Le, Sebastian Lubos",(missing journal),2023,"In many scenarios, configurators support the configuration of a solution that satisfies the preferences of a single user. The concept of \emph{multi-configuration} is based on the idea of configuring a set of configurations. Such a functionality is relevant in scenarios such as the configuration of personalized exams, the configuration of project teams, and the configuration of different trips for individual members of a tourist group (e.g., when visiting a specific city). In this paper, we exemplify the application of multi-configuration for generating individualized exams. We also provide a constraint solver performance analysis which helps to gain some insights into corresponding performance issues.",265339516,0,17,,
(missing DOI),"Feature-Oriented Defect Prediction: Scenarios, Metrics, and Classifiers","M. Mukelabai, Stefan StrÃ¼der, D. StrÃ¼ber, T. Berger",ArXiv,2021,"Software defects are a major nuisance in software development and can lead to considerable financial losses or reputation damage for companies. To this end, a large number of techniques for predicting software defects, largely based on machine learning methods, has been developed over the past decades. These techniques usually rely on code-structure and process metrics to predict defects at the granularity of typical software assets, such as subsystems, components, and files. In this paper, we systematically investigate feature-oriented defect prediction: predicting defects at the granularity of featuresâdomain-entities that abstractly represent software functionality and often cross-cut software assets. Feature-oriented prediction can be beneficial, since: (i) particular features might be more error-prone than others, (ii) characteristics of features known as defective might be useful to predict other error-prone features, and (iii) feature-specific code might be especially prone to faults arising from feature interactions. We explore the feasibility and solution space for feature-oriented defect prediction. We design and investigate scenarios, metrics, and classifiers. Our study relies on 12 software projects from which we analyzed 13,685 bug-introducing and corrective commits, and systematically generated 62,868 training and test datasets to evaluate the designed classifiers, metrics, and scenarios. The datasets were generated based on the 13,685 commits, 81 releases, and 24, 532 permutations of our 12 projects depending on the scenario addressed. We covered scenarios, such as just-in-time (JIT) and cross-project defect prediction. Our results confirm the feasibility of feature-oriented defect prediction. We found the best performance (i.e., precision and robustness) when using the Random Forest classifier, with process and structure metrics. Surprisingly, we found high performance for single-project JIT (median AUROC â¥ 95%) and release-level (median AUROC â¥ 90%) defect predictionâcontrary to studies that assert poor performance due to insufficient training data. Lastly, we found that a model trained on release-level data from one of the twelve projects could predict defect-proneness of features in the other eleven projects with median performance of 82 %, without retraining on the target projects. Our results suggest potential for defect-prediction model-reuse across projects, as well as more reliable defect predictions for developers as they modify or release software features.",231072446,0,74,,
10.1109/ase56229.2023.00091,CoMSA: A Modeling-Driven Sampling Approach for Configuration Performance Testing,"Yuanjie Xia, Zishuo Ding, Weiyi Shang",International Conference on Automated Software Engineering,2023,"Highly configurable systems enable customers to flexibly configure the systems in diverse deployment environments. The flexibility of configurations also poses challenges for performance testing. On one hand, there exist a massive number of possible configurations; while on the other hand, the time and resources are limited for performance testing, which is already a costly process during software development. Modeling the performance of configurations is one of the solutions to reduce the cost of configuration performance testing. Although prior research proposes various modeling and sampling techniques to build configuration performance models, the sampling approaches used in the model typically do not consider the accuracy of the performance models, leading to potential sub-optimal performance modeling results in practice. In this paper, we present a modeling-driven sampling approach (CoMSA) to improve the performance modeling of highly configurable systems. The intuition of CoMSA is to select samples based on their uncertainties to the performance models. In other words, the configurations that have the more uncertain performance prediction results by the performance models are more likely to be selected as further training samples to improve the model. CoMSA is designed by considering both scenarios where 1) the software projects do not have historical performance testing results (cold start) and 2) there exist historical performance testing results (warm start). We evaluate the performance of our approach in four subjects, namely LRZIP, LLVM, x264, and SQLite. Through the evaluation result, we can conclude that our sampling approaches could highly enhance the accuracy of the prediction models and the efficiency of configuration performance testing compared to other baseline sampling approaches.",266042371,1,45,,
10.1145/3579027.3608972,On Programming Variability with Large Language Model-based Assistant,"Mathieu Acher, JosÃ© Galindo, Jean-Marc JÃ©zÃ©quel",Software Product Lines Conference,2023,"Programming variability is central to the design and implementation of software systems that can adapt to a variety of contexts and requirements, providing increased flexibility and customization. Managing the complexity that arises from having multiple features, variations, and possible configurations is known to be highly challenging for software developers. In this paper, we explore how large language model (LLM)-based assistants can support the programming of variability.We report on new approaches made possible with LLM-based assistants, like: features and variations can be implemented as prompts; augmentation of variability out of LLM-based domain knowledge; seamless implementation of variability in different kinds of artefacts, programming languages, and frameworks, at different binding times (compile-time or run-time). We are sharing our data (prompts, sessions, generated code, etc.) to support the assessment of the effectiveness and robustness of LLMs for variability-related tasks.",264331882,8,62,,
(missing DOI),Performance is not Boolean: Supporting Scalar Configuration Variables in NFP Models,"Daniel Friesel, O. Spinczyk",(missing journal),2022,"Non-functional properties (NFPs) such as memory require-ments, timing, or energy consumption are important charac-teristics of embedded software systems and software product lines (SPLs) in general. Both during system design and at runtime, the goal is to optimize resource utilization (and, thus, NFPs) by appropriate system configuration or orchestration. NFP models, learned from benchmarks of various SPL configurations, allow for the prediction of these properties, thus enabling NFP-aware software configuration and runtime decisions. However, many existing approaches for automated learning of NFP models limit their scope to boolean variables. We argue that this is no longer sufficient: NFP models must accommodate scalar variables to achieve suitable accuracy when faced with todayâs highly configurable software systems and variable workloads. To this end, we evaluate four regression tree-based NFP modeling approaches on eight use cases, and examine model complexity and model accuracy. We find that models with support for scalar variables achieve up to three times lower mean model error when predicting configurations that were not part of the training set. At the same time, the complexity of scalar and boolean-only models is nearly the same; only benchmarking becomes more time-intensive due to the need to explore scalar variables. We conclude that scalar-enabled models provide increased accuracy almost free of charge, and recommend using them when generating NFP models for embedded systems and workloads with scalar configuration variables.",238228667,1,19,,
10.1145/3510003.3510190,On the benefits and limits of incremental build of software configurations,"Georges Aaron Randrianaina, Xhevahire TÃ«rnava, Djamel Eddine Khelladi, Mathieu Acher, Georges Aaron Randrianaina, Xhevahire TÃ«rnava, Djamel Eddine Khelladi, Mathieu Acher",Proceedings of the 44th International Conference on Software Engineering,2022,(missing abstract),252619611,3,36,,
10.1145/3510003.3510190,On the benefits and limits of incremental build of software configurations: an exploratory study,"Georges Aaron Randrianaina, Xhevahire TÃ«rnava, D. Khelladi, M. Acher",Proceedings of the 44th International Conference on Software Engineering,2022,"Software projects use build systems to automate the compilation, testing, and continuous deployment of their software products. As software becomes increasingly configurable, the build of multiple configurations is a pressing need, but expensive and challenging to implement. The current state of practice is to build independently (a.k.a., clean build) a software for a subset of configurations. While incremental build has been studied for software evolution and relatively small changes of the source code, it has surprisingly not been considered for software configurations. In this exploratory study, we examine the benefits and limits of building software configurations incrementally, rather than always building them cleanly. By using five real-life configurable systems as subjects, we explore whether incremental build works, outperforms a sequence of clean builds, is correct w.r.t. clean build, and can be used to find an optimal ordering for building configurations. Our results show that incremental build is feasible in 100% of the times in four subjects and in 78% of the times in one subject. In average, 88.5% of the configurations could be built faster with incremental build while also finding several alternatives faster incremental builds. However, only 60% of faster incremental builds are correct. Still, when considering those correct incremental builds with clean builds, we could always find an optimal order that is faster than just a collection of clean builds with a gain up to 11.76%.",264785775,8,44,,
10.1109/tsc.2022.3142853,FOCloud: Feature Model Guided Performance Prediction and Explanation for Deployment Configurable Cloud Applications,"I. Kumara, Mohamed Hameez Ariz, Mohan Baruwal Chhetri, Majid Mohammadi, Willem-Jan van den Heuvel, D. Tamburri",IEEE Transactions on Services Computing,2023,"The increasing heterogeneity of the VM offerings on public IaaS clouds gives rise to a very large number of deployment options for constructing distributed, multi-component cloud applications. However, selecting an appropriate deployment variant, i.e., a valid combination of deployment options, to meet required performance levels is non-trivial. The combinatorial explosion of the deployment space makes it infeasible to measure the performance of all deployment variants to build a comprehensive empirical performance model. To address this problem, we propose Feature-Oriented Cloud (FOCloud), a performance engineering approach for deployment configurable cloud applications. FOCloud (i) uses feature modeling to structure and constrain the valid deployment space by modeling the commonalities and variations in the different deployment options and their inter-dependencies, (ii) uses sampling and machine learning to incrementally and cost-effectively build a performance prediction model whose input variables are the deployment options, and the output variable is the performance of the resulting deployment variant, and (iii) uses Explainable AI techniques to provide explanations for the prediction outcomes of valid deployment variants in terms of the deployment options. We demonstrate the practicality and feasibility of FOCloud by applying it to an extension of the RuBiS benchmark application deployed on Google Cloud.",175060768,11,36,,
10.1145/3382025.3414960,Feature-oriented defect prediction,"Stefan StrÃ¼der, M. Mukelabai, D. StrÃ¼ber, T. Berger",Software Product Lines Conference,2020,"Software errors are a major nuisance in software development and can lead not only to reputation damages, but also to considerable financial losses for companies. Therefore, numerous techniques for predicting software defects, largely based on machine learning methods, have been developed over the past decades. These techniques usually rely on code and process metrics in order to predict defects at the granularity of typical software assets, such as subsystems, components, and files. In this paper, we present the first systematic investigation of feature-oriented defect prediction: the prediction of defects at the granularity of features---domain-oriented entities abstractly representing (and often cross-cutting) typical software assets. Feature-oriented prediction can be beneficial, since: (i) particular features might be more error-prone than others, (ii) characteristics of features known as defective might be useful to predict other error-prone features, (iii) feature-specific code might be especially prone to faults arising from feature interactions. We present a dataset derived from 12 software projects and introduce two metric sets for feature-oriented defect prediction. We evaluated seven machine learning classifiers with three different attribute sets each, using our two new metric sets as well as an existing metric set from the literature. We observe precision and recall values of around 85% and better robustness when more diverse metrics sets with richer feature information are used.",21556533,23,92,,
10.1145/3672555,Paving a Path for a Combined Family of Feature Toggle and Configuration Option Research,"Rezvan Mahdavi-Hezaveh, Sameeha Fatima, Laurie Williams",ACM Transactions on Software Engineering and Methodology,2024,"Feature toggles and configuration options are techniques to include or exclude functionality in software. The research contributions to these two techniques have most often been focused on either one of them. However, focusing on the similarities of these two techniques and the use of a common terminology may enable a combined family of research on software configuration (a term we use to encompass both techniques) and prevent duplication of effort. The goal of this study is to aid researchers in conducting a family of research on software configuration by extending an existing model of software configuration that provides a common terminology for feature toggles and configuration options in research studies. We started with Siegmund et al.âs Model of Software Configuration (MSC), which was developed based on configuration option-related resources. We extend the MSC by qualitative analysis of feature toggle-related resources. From our analysis, we proposed MSCv2 and evaluated it through its application on publications and an industrial system. Our results indicate researchers studying the same system may provide different definitions of software configuration in publications, similar research questions may be answered repeatedly because of a lack of a clear definition of software configuration, and having a model of software configuration may enable generalized research on this family of research.1",273166423,0,64,,
10.1007/978-3-031-15629-8_8,Family-Based Fingerprint Analysis: A Position Paper,"Carlos Diego Nascimento Damasceno, Daniel StrÃ¼ber",(missing journal),2022,"Thousands of vulnerabilities are reported on a monthly basis to security repositories, such as the National Vulnerability Database. Among these vulnerabilities, software misconfiguration is one of the top 10 security risks for web applications. With this large influx of vulnerability reports, software fingerprinting has become a highly desired capability to discover distinctive and efficient signatures and recognize reportedly vulnerable software implementations. Due to the exponential worst-case complexity of fingerprint matching, designing more efficient methods for fingerprinting becomes highly desirable, especially for variability-intensive systems where optional features add another exponential factor to its analysis. This position paper presents our vision of a framework that lifts model learning and family-based analysis principles to software fingerprinting. In this framework, we propose unifying databases of signatures into a featured finite state machine and using presence conditions to specify whether and in which circumstances a given input-output trace is observed. We believe feature-based signatures can aid performance improvements by reducing the size of fingerprints under analysis.",251699649,2,37,,
10.1016/j.jss.2023.111671,Input sensitivity on the performance of configurable systems an empirical study,"Luc Lesoil, M. Acher, Arnaud Blouin, Jean-Marc J'ez'equel",Journal of Systems and Software,2021,(missing abstract),255419318,12,109,,
10.1145/3646548.3672586,Pragmatic Random Sampling of the Linux Kernel: Enhancing the Randomness and Correctness of the conf Tool,"David FernÃ¡ndez-AmorÃ³s, RubÃ©n Heradio, JosÃ©-Miguel Horcas, JosÃ© Ã. Galindo, David Benavides, Lidia Fuentes",Software Product Lines Conference,2024,(missing abstract),275865501,0,19,,
10.1145/3510457.3513035,"Code Reviewer Recommendation in Tencent: Practice, Challenge, and Direction*","Qiuyuan Chen, Dezhen Kong, Lingfeng Bao, Chenxing Sun, Xin Xia, Shanping Li",2022 IEEE/ACM 44th International Conference on Software Engineering: Software Engineering in Practice (ICSE-SEIP),2022,"Code review is essential for assuring system quality in software engineering. Over decades in practice, code review has evolved to be a lightweight tool-based process focusing on code change: the smallest unit of the development cycle, and we refer to it as Modern Code Review (MCR). MCR involves code contributors committing code changes and code reviewers reviewing the assigned code changes. Such a reviewer assigning process is challenged by efficiently finding appropriate reviewers. Recent studies propose automated code reviewer recommendation (CRR) approaches to resolve such challenges. These approaches are often evaluated on open-source projects and obtain promising performance. However, the code reviewer recommendation systems are not widely used on proprietary projects, and most current reviewer selecting practice is still manual or, at best, semi-manual. No previous work systematically evaluated these approachesâ effectiveness and compared each other on proprietary projects in practice. In this paper, we performed a quantitative analysis of typical recommendation approaches on proprietary projects in Tencent. The results show an imperfect performance of these approaches on proprietary projects and reveal practical challenges like the âcold start problemâ. To better understand practical challenges, we interviewed practitioners about the expectations of applying reviewer recommendations to a production environment. The interview involves the current systemsâ limitations, expected application scenario, and information requirements. Finally, we discuss the implications and the direction of practical code reviewer recommendation tools.",169334226,10,87,,
10.1145/3377024.3377040,Generating attributed variability models for transfer learning,"Johannes Dorn, S. Apel, Norbert Siegmund",VaMoS,2020,"Modern software systems often provide configuration options for customizing of the system's functional and non-functional properties, such as response time and energy consumption. The valid configurations of a software system are commonly documented in a variability model. Supporting the optimization of a system's non-functional properties, variability models have been extended with attributes that represent the influence of one or multiple options on a property. The concrete values of attributes are typically determined only in a single environment (e.g., for a specific software version, a certain workload, and a specific hardware setup) and are applicable only for this context. Changing the environment, attribute values need to be updated. Instead of determining all attributes from scratch with new measurements, recent approaches rely on transfer learning to reduce the effort of obtaining new attribute values. However, the development and evaluation of new transfer-learning techniques requires extensive measurements by themselves, which often is prohibitively costly. To support research in this area, we propose an approach to synthesize realistic attributed variability models from a base model. This way, we can support research and validation of novel transfer-learning techniques for configurable software systems. We use a genetic algorithm to vary attribute values. Combined with a declarative objective function, we search a changed attributed variability model that keeps some key characteristics while mimicking realistic changes of individual attribute values. We demonstrate the applicability of our approach by replicating the evaluation of an existing transfer-learning technique.",140749632,8,32,,
10.1145/3624007.3624058,Unleashing the Power of Implicit Feedback in Software Product Lines: Benefits Ahead,"Raul Medeiros, Oscar DÃ­az, David Benavides",International Conference on Generative Programming: Concepts and Experiences,2023,"Software Product Lines (SPLs) facilitate the development of a complete range of software products through systematic reuse. Reuse involves not only code but also the transfer of knowledge gained from one product to others within the SPL. This transfer includes bug fixing, which, when encountered in one product, affects the entire SPL portfolio. Similarly, feedback obtained from the usage of a single product can inform beyond that product to impact the entire SPL portfolio. Specifically, implicit feedback refers to the automated collection of data on software usage or execution, which allows for the inference of customer preferences and trends. While implicit feedback is commonly used in single-product development, its application in SPLs has not received the same level of attention. This paper promotes the investigation of implicit feedback in SPLs by identifying a set of SPL activities that can benefit the most from it. We validate this usefulness with practitioners using a questionnaire-based approach (n=8). The results provide positive insights into the advantages and practical implications of adopting implicit feedback at the SPL level.",265461275,0,32,,
10.1145/3358960.3379137,Sampling Effect on Performance Prediction of Configurable Systems: A Case Study,"Juliana Alves Pereira, M. Acher, Hugo Martin, J. JÃ©zÃ©quel",International Conference on Performance Engineering,2020,"Numerous software systems are highly configurable and provide a myriad of configuration options that users can tune to fit their functional and performance requirements (e.g., execution time). Measuring all configurations of a system is the most obvious way to understand the effect of options and their interactions, but is too costly or infeasible in practice. Numerous works thus propose to measure only a few configurations (a sample) to learn and predict the performance of any combination of options' values. A challenging issue is to sample a small and representative set of configurations that leads to a good accuracy of performance prediction models. A recent study devised a new algorithm, called distance-based sampling, that obtains state-of-the-art accurate performance predictions on different subject systems. In this paper, we replicate this study through an in-depth analysis of x264, a popular and configurable video encoder. We systematically measure all 1,152 configurations of x264 with 17 input videos and two quantitative properties (encoding time and encoding size). Our goal is to understand whether there is a dominant sampling strategy over the very same subject system (x264), i.e., whatever the workload and targeted performance properties. The findings from this study show that random sampling leads to more accurate performance models. However, without considering random, there is no single ""dominant"" sampling, instead different strategies perform best on different inputs and non-functional properties, further challenging practitioners and researchers.",68890942,53,79,,
10.1145/3572905,Machine Learning for Software Engineering: A Tertiary Study,"Zoe Kotti, Rafaila Galanopoulou, Diomidis Spinellis, Zoe Kotti, Rafaila Galanopoulou, Diomidis Spinellis",ACM Computing Surveys,2022,"Machine learning (ML) techniques increase the effectiveness of software engineering (SE) lifecycle activities. We systematically collected, quality-assessed, summarized, and categorized 83 reviews in ML for SE published between 2009â2022, covering 6 117 primary studies. The SE areas most tackled with ML are software quality and testing, while human-centered areas appear more challenging for ML. We propose a number of ML for SE research challenges and actions including: conducting further empirical validation and industrial studies on ML; reconsidering deficient SE methods; documenting and automating data collection and pipeline processes; reexamining how industrial practitioners distribute their proprietary data; and implementing incremental ML approaches.",251122534,24,157,,
(missing DOI),Bayesian Learning for Hardware and Software Conï¬guration Co-Optimization,Yi Ding,(missing journal),2020,"Both hardware and software systems are increasingly configurable, which poses a challenge to finding the highest performance configuration due to the tremendous search space. Prior scheduling and resource management work uses machine learning (ML) to find high-performance configurations for either the hardware or software configurations alone while assuming the other is fixed. Such separate optimization is problematic because a software configuration that is fast on one hardware architecture can be up to 50% slower on another. The main difficulty of co-optimization is the massive search space over both hardware and software configurations because accurate learning models require large amounts of labeled training data, and thus long periods of data collection and configuration search. To achieve co-optimization with significantly smaller training sets, we present Paprika, a scheduler that simultaneously selects hardware and software configurations using a Bayesian learning approach. Paprika augments software configuration parameters with hardware features seamlessly and efficiently via one-hot encoding and parameter selection. To reduce the impact of the search space, Paprika actively queries configurations based on a novel ensemble optimization objective. The intuition of this ensemble is to combine three prior optimizers to implicitly negotiate the explorationâexploitation tradeoff. We evaluate Paprika with ten Spark workloads on three hardware architectures and find that, compared to prior work, Paprika produces runtimes that are 12â38% closer to the optimal.",269894043,5,75,,
10.1145/3641525.3663621,Embracing Deep Variability For Reproducibility and Replicability,"Mathieu Acher, BenoÃ®t Combemale, Georges Aaron Randrianaina, J. JÃ©zÃ©quel",ACM-REP,2024,"Reproducibility ( a.k.a. , determinism in some cases) constitutes a fundamental aspect in various fields of computer science, such as floating-point computations in numerical analysis and simulation, concurrency models in parallelism, reproducible builds for third parties integration and packaging, and containerization for execution environments. These concepts, while pervasive across diverse concerns, often exhibit intricate inter-dependencies, making it challenging to achieve a comprehensive understanding. In this short and vision paper we delve into the application of software engineering techniques, specifically variability management, to systematically identify and explicit points of variability that may give rise to reproducibility issues ( e.g. , language, libraries, compiler, virtual machine, OS, environment variables, etc. ). The primary objectives are: i) gaining insights into the variability layers and their possible interactions, ii) capturing and documenting configurations for the sake of reproducibility, and iii) exploring diverse configurations to replicate, and hence validate and ensure the robustness of results. By adopting these methodologies, we aim to address the complexities associated with reproducibility and replicability in modern software systems and environments, facilitating a more comprehensive and nuanced perspective on these critical aspects.",273221787,0,44,,
10.1007/978-3-031-53966-4_25,Learning Graph Configuration Spaces with Graph Embedding in Engineering Domains,"Michael Mittermaier, Takfarinas Saber, Goetz Botterweck",Lecture Notes in Computer Science,2024,(missing abstract),269342363,0,14,,
10.1145/3461001.3471149,The interplay of compile-time and run-time options for performance prediction,"Luc Lesoil, M. Acher, Xhevahire TÃ«rnava, Arnaud Blouin, J. JÃ©zÃ©quel",Software Product Lines Conference,2021,"Many software projects are configurable through compile-time options (e.g., using ./configure) and also through run-time options (e.g., command-line parameters, fed to the software at execution time). Several works have shown how to predict the effect of run-time options on performance. However it is yet to be studied how these prediction models behave when the software is built with different compile-time options. For instance, is the best run-time configuration always the best w.r.t. the chosen compilation options? In this paper, we investigate the effect of compile-time options on the performance distributions of 4 software systems. There are cases where the compiler layer effect is linear which is an opportunity to generalize performance models or to tune and measure runtime performance at lower cost. We also prove there can exist an interplay by exhibiting a case where compile-time options significantly alter the performance distributions of a configurable system.",216791337,13,62,,
10.1145/3551349.3559499,Automated Identification of Security-Relevant Configuration Settings Using NLP,"Patrick StÃ¶ckle, Theresa Wasserer, Bernd Grobauer, A. Pretschner",ArXiv,2022,"To secure computer infrastructure, we need to configure all security-relevant settings. We need security experts to identify security-relevant settings, but this process is time-consuming and expen-sive. Our proposed solution uses state-of-the-art natural language processing to classify settings as security-relevant based on their description. Our evaluation shows that our trained classifiers do not perform well enough to replace the human security experts but can help them classify the settings. By publishing our labeled data sets and the code of our trained model, we want to help security experts analyze configuration settings and enable further research in this area.",251941278,4,11,,
10.48550/arxiv.2403.03322,Deep Configuration Performance Learning: A Systematic Survey and Taxonomy,"Jingzhi Gong, Tao Chen",arXiv.org,2024,"
 Performance is arguably the most crucial attribute that reflects the quality of a configurable software system. However, given the increasing scale and complexity of modern software, modeling and predicting how various configurations can impact performance becomes one of the major challenges in software maintenance. As such, performance is often modeled without having a thorough knowledge of the software system, but relying mainly on data, which fits precisely with the purpose of deep learning. In this paper, we conduct a comprehensive review exclusively on the topic of deep learning for performance learning of configurable software, covering 1,206 searched papers spanning six indexing services, based on which 99 primary papers were extracted and analyzed. Our results outline key statistics, taxonomy, strengths, weaknesses, and optimal usage scenarios for techniques related to the preparation of configuration data, the construction of deep learning performance models, the evaluation of these models, and their utilization in various software configuration-related tasks. We also identify the good practices and potentially problematic phenomena from the studies surveyed, together with a comprehensive summary of actionable suggestions and insights into future opportunities within the field. To promote open science, all the raw results of this survey can be accessed at our repository:
 https://github.com/ideas-labo/DCPL-SLR
 .
",270316404,3,164,,
10.1145/3572905,Machine Learning for Software Engineering: A Tertiary Study,"Zoe Kotti, R. Galanopoulou, D. Spinellis",ACM Computing Surveys,2022,"Machine learning (ML) techniques increase the effectiveness of software engineering (SE) lifecycle activities. We systematically collected, quality-assessed, summarized, and categorized 83 reviews in ML for SE published between 2009 and 2022, covering 6,117 primary studies. The SE areas most tackled with ML are software quality and testing, while human-centered areas appear more challenging for ML. We propose a number of ML for SE research challenges and actions, including conducting further empirical validation and industrial studies on ML, reconsidering deficient SE methods, documenting and automating data collection and pipeline processes, reexamining how industrial practitioners distribute their proprietary data, and implementing incremental ML approaches.",277685843,19,184,,
10.1145/3546932.3546991,Adaptive behavioral model learning for software product lines,"Shaghayegh Tavassoli, C. Damasceno, R. Khosravi, M. Mousavi",Software Product Lines Conference,2022,"Behavioral models enable the analysis of the functionality of software product lines (SPL), e.g., model checking and model-based testing. Model learning aims to construct behavioral models. Due to the commonalities among the products of an SPL, it is possible to reuse the previously-learned models during the model learning process. In this paper, an adaptive approach, called PL*, for learning the product models of an SPL is presented based on the well-known L* algorithm. In this method, after learning each product, the sequences in the final observation table are stored in a repository which is used to initialize the observation table of the remaining products. The proposed algorithm is evaluated on two open-source SPLs and the learning cost is measured in terms of the number of rounds, resets, and input symbols. The results show that for complex SPLs, the total learning cost of PL* is significantly lower than that of the non-adaptive method in terms of all three metrics. Furthermore, it is observed that the order of learning products affects the efficiency of PL*. We introduce a heuristic to determine an ordering which reduces the total cost of adaptive learning.",237506193,12,50,,
10.48550/arxiv.2210.14082,Specialization of Run-time Configuration Space at Compile-time: An Exploratory Study,"Xhevahire TÃ«rnava, M. Acher, B. Combemale",ArXiv,2022,"Numerous software systems are highly configurable through run-time options, such as command-line parameters. Users can tune some of the options to meet various functional and non-functional requirements such as footprint, security, or execution time. However, some options are never set for a given system instance, and their values remain the same whatever the use cases of the system. Herein, we design a controlled experiment in which the systemâs run-time configuration space can be specialized at compile-time and com-binations of options can be removed on demand. We perform an in-depth study of the well-known x264 video encoder and quantify the effects of its specialization to its non-functional properties, namely on binary size, attack surface, and performance while ensuring its validity. Our exploratory study suggests that the configurable specialization of a system has statistically significant benefits on most of its analysed non-functional properties, which benefits depend on the number of the debloated options. While our empirical results and insights show the importance of removing code related to unused run-time options to improve software systems, an open challenge is to further automate the specialization process.",264434088,0,61,,
10.1016/j.jss.2022.111539,Automating Feature Model maintainability evaluation using machine learning techniques,"PÃºblio Silva, Carla Ilane Moreira Bezerra, Ivan do Carmo Machado, PÃºblio Silva, Carla Ilane Moreira Bezerra, Ivan do Carmo Machado",(missing journal),2022,(missing abstract),251579609,5,48,,
10.1145/3579027.3608989,True Variability Shining Through Taxonomy Mining,"C. KÃ¶nig, Kamil Rosiak, L. Cleophas, Ina Schaefer",Software Product Lines Conference,2023,"Software variants of a Software Product Line (SPL) consist of a set of artifacts specified by features. Variability models document the valid relationships between features and their mapping to artifacts. However, research has shown inconsistencies between the variability of variants in features and artifacts, with negative effects on system safety and development effort. To analyze this mismatch in variability, the causal relationships between features, artifacts, and variants must be uncovered, which has only been addressed to a limited extent. In this paper, we propose taxonomy graphs as novel variability models that reflect the composition of variants from artifacts and features, making mismatches in variability explicit. Our evaluation with two SPL case studies demonstrates the usefulness of our variability model and shows that mismatches in variability can vary significantly in detail and severity.",264226648,2,75,,
10.1145/3492321.3519575,Unicorn: reasoning about configurable system performance through the lens of causality,"Md Shahriar Iqbal, R. Krishna, Mohammad Ali Javidian, Baishakhi Ray, Pooyan Jamshidi",European Conference on Computer Systems,2022,"Modern computer systems are highly configurable, with the total variability space sometimes larger than the number of atoms in the universe. Understanding and reasoning about the performance behavior of highly configurable systems, over a vast and variable space, is challenging. State-of-the-art methods for performance modeling and analyses rely on predictive machine learning models, therefore, they become (i) unreliable in unseen environments (e.g., different hardware, workloads), and (ii) may produce incorrect explanations. To tackle this, we propose a new method, called Unicorn, which (i) captures intricate interactions between configuration options across the software-hardware stack and (ii) describes how such interactions can impact performance variations via causal inference. We evaluated Unicorn on six highly configurable systems, including three on-device machine learning systems, a video encoder, a database management system, and a data analytics pipeline. The experimental results indicate that Unicorn outperforms state-of-the-art performance debugging and optimization methods in finding effective repairs for performance faults and finding configurations with near-optimal performance. Further, unlike the existing methods, the learned causal performance models reliably predict performance for new environments.",244185068,22,97,,
10.5753/sbsi_estendido.2024.238518,Effect of Feature Subset Selection on Samplings for Performance Prediction of Configurable Systems,"JoÃ£o Marcello Bessa Rodrigues, Juliana Alves Pereira",Brazilian Symposium on Information Systems,2024,"Organizations require personalized solutions to effectively address usersâ needs, and stay competitive in the market. In this context, configurable systems offer numerous configuration options to meet user-specific functional and non-functional requirements. However, although configurability makes these systems flexible and versatile, a simple change can result in serious problems in different software variants, such as performance bottlenecks and security issues. Thus, automated approaches based on machine learning have been developed to facilitate configuration management. Our work aims to expand upon previous findings in this field by assessing their applicability to other scenarios. By introducing more efficient practices, we can contribute to cost reduction, higher software quality, and quicker time-to-market. This is particularly relevant in a global context where software plays a crucial role.",273921575,0,18,,
10.1145/3579028.3609016,Generative AI for Reengineering Variants into Software Product Lines,"Mathieu Acher, Jabier Martinez",(missing journal),2023,"The migration and reengineering of existing variants into a software product line (SPL) is an error-prone and time-consuming activity. Many extractive approaches have been proposed, spanning different activities from feature identification and naming to the synthesis of reusable artefacts. In this paper, we explore how large language model (LLM)-based assistants can support domain analysts and developers. We revisit four illustrative cases of the literature where the challenge is to migrate variants written in different formalism (UML class diagrams, Java, GraphML, statecharts). We systematically report on our experience with ChatGPT-4, describing our strategy to prompt LLMs and documenting positive aspects but also failures. We compare the use of LLMs with state-of-the-art approach, BUT4Reuse. While LLMs offer potential in assisting domain analysts and developers in transitioning software variants into SPLs, their intrinsic stochastic nature and restricted ability to manage large variants or complex structures necessitate a semiautomatic approach, complete with careful review, to counteract inaccuracies.",264284883,1,24,,
10.1145/3510455.3512792,Towards Incremental Build of Software Configurations,"Georges Aaron Randrianaina, D. Khelladi, Olivier Zendra, M. Acher",2022 IEEE/ACM 44th International Conference on Software Engineering: New Ideas and Emerging Results (ICSE-NIER),2022,"Building software is a crucial task to compile, test, and deploy software systems while continuously ensuring quality. As software is more and more configurable, building multiple configurations is a pressing need, yet, costly and challenging to instrument. The common practice is to independently build (a.k.a., clean build) a software for a subset of configurations. While incremental build has been considered for software evolution and relatively small modifications of the source code, it has surprisingly not been considered for software configurations. In this vision paper, we formulate the hypothesis that incremental build can reduce the cost of exploring the configuration space of software systems. We detail how we apply incremental build for two real-world application scenarios and conduct a preliminary evaluation on two case studies, namely x264 and Linux Kernel. For x264, we found that one can incrementally build configurations in an order such that overall build time is reduced. Nevertheless, we could not find any optimal order with the Linux Kernel, due to a high distance between random configurations. Therefore, we show it is possible to control the process of generating configurations: we could reuse commonality and gain up to 66% of build time compared to only clean builds.CCS CONCEPTS â¢ Software and its engineering $\rightarrow$ Software configuration management and version control systems.",262473351,7,40,,
10.1109/tse.2021.3116768,Transfer Learning Across Variants and Versions: The Case of Linux Kernel Size,"Hugo Martin, M. Acher, Juliana Alves Pereira, Luc Lesoil, J. JÃ©zÃ©quel, D. Khelladi",IEEE Transactions on Software Engineering,2021,"With large scale and complex configurable systems, it is hard for users to choose the right combination of options (i.e., configurations) in order to obtain the wanted trade-off between functionality and performance goals such as speed or size. Machine learning can help in relating these goals to the configurable system options, and thus, predict the effect of options on the outcome, typically after a costly training step. However, many configurable systems evolve at such a rapid pace that it is impractical to retrain a new model from scratch for each new version. In this paper, we propose a new method to enable transfer learning of binary size predictions among versions of the same configurable system. Taking the extreme case of the Linux kernel with its <inline-formula><tex-math notation=""LaTeX"">$\approx 14,500$</tex-math><alternatives><mml:math><mml:mrow><mml:mo>â</mml:mo><mml:mn>14</mml:mn><mml:mo>,</mml:mo><mml:mn>500</mml:mn></mml:mrow></mml:math><inline-graphic xlink:href=""martin-ieq1-3116768.gif""/></alternatives></inline-formula> configuration options, we first investigate how binary size predictions of kernel size degrade over successive versions. We show that the direct reuse of an accurate prediction model from 2017 quickly becomes inaccurate when Linux evolves, up to a 32% mean error by August 2020. We thus propose a new approach for transfer evolution-aware model shifting (<sc>tEAMS</sc>). It leverages the structure of a configurable system to transfer an initial predictive model towards its future versions with a minimal amount of extra processing for each version. We show that <sc>tEAMS</sc> vastly outperforms state of the art approaches over the 3 years history of Linux kernels, from 4.13 to 5.8.",262623393,27,87,,
10.1016/j.jss.2024.112316,CSAT: Configuration structure-aware tuning for highly configurable software systems,"Yufei Li, Liang Bao, Kaipeng Huang, C. Wu",Journal of Systems and Software,2024,(missing abstract),282059539,0,48,,
(missing DOI),ACTIVITY REPORT Project-Team DIVERSE,"BenoÃ®t Combemale, Arnaud Blouin, David Mendez Acuna, D. Vojtisek, Dorian Leroy, Erwan Bousse, F. Coulon, J. JÃ©zÃ©quel, Olivier Barais, Thomas Degueule, DSpot, Benjamin Danglot, Benoit Baudry, M. Monperrus, Descartes, Oscar Luis, Vera Perez",(missing journal),2022,"Functional Description: Familiar is an environment for large-scale product customisation. From a model of product features (options, parameters, etc.), Familiar can automatically generate several million variants. These variants can take many forms: software, a graphical interface, a video sequence or even a manufactured product (3D printing). Familiar is particularly well suited for developing web conï¬gurators (for ordering customised products online), for providing online comparison tools and also for engineering any family of embedded or software-based products.",270048760,0,120,,
10.1145/3442391.3442402,Deep Software Variability: Towards Handling Cross-Layer Configuration,"Luc Lesoil, M. Acher, Arnaud Blouin, J. JÃ©zÃ©quel",International Workshop on Variability Modelling of Software-Intensive Systems,2021,"Configuring software is a powerful means to reach functional and performance goals of a system. However, many layers (hardware, operating system, input data, etc.), themselves subject to variability, can alter performances of software configurations. For instance, configurationsâ options of the x264 video encoder may have very different effects on x264âs encoding time when used with different input videos, depending on the hardware on which it is executed. In this vision paper, we coin the term deep software variability to refer to the interaction of all external layers modifying the behavior or non-functional properties of a software. Deep software variability challenges practitioners and researchers: the combinatorial explosion of possible executing environments complicates the understanding, the configuration, the maintenance, the debug, and the test of configurable systems. There are also opportunities: harnessing all variability layers (and not only the software layer) can lead to more efficient systems and configuration knowledge that truly generalizes to any usage and context.",122204599,15,47,,
10.1145/3503229.3547067,Acapulco: an extensible tool for identifying optimal and consistent feature model configurations,"Jabier Martinez, D. StrÃ¼ber, J. Horcas, Alexandru Burdusel, S. Zschaler",Software Product Lines Conference,2022,"Configuring feature-oriented variability-rich systems is complex because of the large number of features and, potentially, the lack of visibility of the implications on quality attributes when selecting certain features. We present Acapulco as an alternative to the existing tools for automating the configuration process with a focus on mono- and multi-criteria optimization. The soundness of the tool has been proven in a previous publication comparing it to SATIBEA and MODAGAME. The main advantage was obtained through consistency-preserving configuration operators (CPCOs) that guarantee the validity of the configurations during the IBEA genetic algorithm evolution process. We present a new version of Acapulco built on top of FeatureIDE, extensible through the easy integration of objective functions, providing pre-defined reusable objectives, and being able to handle complex feature model constraints.",233233635,4,15,,
10.1007/978-3-031-34560-9_4,Configuration Optimization with Limited Functional Impact,"Edouard GuÃ©gain, Amir Taherkordi, ClÃ©ment Quinton",Lecture Notes in Computer Science,2023,"Dealing with a large configuration space is a complex task for developers, especially when configurations must comply with both functional constraints and non-functional goals. In this paper, we introduce an approach to optimize any set of performance indicators for an existing configuration, while meeting functional requirements. The efficiency of this approach is assessed by exhaustively optimizing a configurable system, and by analyzing how the algorithm navigates through the configuration space. This approach proves especially efficient at optimizing configurations through a minimal number of changes, thus limiting the impact on their functional behavior.",262362324,1,11,,
(missing DOI),FlexiBO: Cost-Aware Multi-Objective Optimization of Deep Neural Networks,"Md Shahriar Iqbal, Jianhai Su, Lars Kotthoff, Pooyan Jamshidi",arXiv.org,2020,"One of the key challenges in designing machine learning systems is to determine the right balance amongst several objectives, which also oftentimes are incommensurable and conflicting. For example, when designing deep neural networks (DNNs), one often has to trade-off between multiple objectives, such as accuracy, energy consumption, and inference time. Typically, there is no single configuration that performs equally well for all objectives. Consequently, one is interested in identifying Pareto-optimal designs. Although different multi-objective optimization algorithms have been developed to identify Pareto-optimal configurations, state-of-the-art multi-objective optimization methods do not consider the different evaluation costs attending the objectives under consideration. This is particularly important for optimizing DNNs: the cost arising on account of assessing the accuracy of DNNs is orders of magnitude higher than that of measuring the energy consumption of pre-trained DNNs. We propose FlexiBO, a flexible Bayesian optimization method, to address this issue. We formulate a new acquisition function based on the improvement of the Pareto hyper-volume weighted by the measurement cost of each objective. Our acquisition function selects the next sample and objective that provides maximum information gain per unit of cost. We evaluated FlexiBO on 7 state-of-the-art DNNs for object detection, natural language processing, and speech recognition. Our results indicate that, when compared to other state-of-the-art methods across the 7 architectures we tested, the Pareto front obtained using FlexiBO has, on average, a 28.44% higher contribution to the true Pareto front and achieves 25.64% better diversity.",247203165,24,75,,
10.48550/arxiv.2409.16480,Exploring Performance Trade-offs in JHipster,"Edouard GuÃ©gain, Alexandre Bonvoisin, ClÃ©ment Quinton, Mathieu Acher, Romain Rouvoy",arXiv.org,2024,"The performance of software systems remains a persistent concern in the field of software engineering. While traditional metrics like binary size and execution time have long been focal points for developers, the power consumption concern has gained significant attention, adding a layer of complexity to performance evaluation. Configurable software systems, with their potential for numerous configurations, further complicate this evaluation process. In this experience paper, we examine the impact of configurations on performance, specifically focusing on the web stack generator JHipster. Our goal is to understand how configuration choices within JHipster influence the performance of the generated system. We undertake an exhaustive analysis of JHipster by examining its configurations and their effects on system performance. Additionally, we explore individual configuration options to gauge their specific influence on performance. Through this process, we develop a comprehensive performance model for JHipster, enabling us to automate the identification of configurations that optimize specific performance metrics. In particular, we identify configurations that demonstrate near-optimal performance across multiple indicators and report on significant correlations between configuration choices within JHipster and the performance of generated systems.",278277696,0,26,,
10.1109/mascots50786.2020.9285960,Baloo: Measuring and Modeling the Performance Configurations of Distributed DBMS,"Johannes Grohmann, Daniel Seybold, Simon Eismann, Mark Leznik, Samuel Kounev, JÃ¶rg Domaschka","IEEE/ACM International Symposium on Modeling, Analysis, and Simulation On Computer and Telecommunication Systems",2020,"Correctly configuring a distributed database management system (DBMS) deployed in a cloud environment for maximizing performance poses many challenges to operators. Even if the entire configuration spectrum could be measured directly, which is often infeasible due to the multitude of parameters, single measurements are subject to random variations and need to be repeated multiple times. In this work, we propose Baloo, a framework for systematically measuring and modeling different performance-relevant configurations of distributed DBMS in cloud environments. Baloo dynamically estimates the required number of measurement configurations, as well as the number of required measurement repetitions per configuration based on a desired target accuracy. We evaluate Baloo based on a data set consisting of 900 DBMS configuration measurements conducted in our private cloud setup. Our evaluation shows that the highly configurable framework is able to achieve a prediction error of up to 12 %, while saving over 80 % of the measurement effort. We also publish all code and the acquired data set to foster future research.",179263520,8,36,,
10.1145/3546932.3546997,Feature subset selection for learning huge configuration spaces: the case of linux kernel size,"M. Acher, Hugo Martin, Luc Lesoil, Arnaud Blouin, J. JÃ©zÃ©quel, D. Khelladi, Olivier Barais, Juliana Alves Pereira",Software Product Lines Conference,2022,"Linux kernels are used in a wide variety of appliances, many of them having strong requirements on the kernel size due to constraints such as limited memory or instant boot. With more than nine thousands of configuration options to choose from, developers and users of Linux actually spend significant effort to document, understand, and eventually tune (combinations of) options for meeting a kernel size. In this paper, we describe a large-scale endeavour automating this task and predicting a given Linux kernel binary size out of unmeasured configurations. We first experiment that state-of-the-art solutions specifically made for configurable systems such as performance-influence models cannot cope with that number of options, suggesting that software product line techniques may need to be adapted to such huge configuration spaces. We then show that tree-based feature selection can learn a model achieving low prediction errors over a reduced set of options. The resulting model, trained on 95 854 kernel configurations, is fast to compute, simple to interpret and even outperforms the accuracy of learning without feature selection.",241034486,7,71,,
10.1145/3660317.3660322,Cost-Efficient Construction of Performance Models,"Larissa Schmid, Timur SaÄlam, Michael Selzer, Anne Koziolek","Proceedings of the 4th Workshop on Performance EngineeRing, Modelling, Analysis, and VisualizatiOn STrategy",2024,(missing abstract),276911266,0,22,,
10.48550/arxiv.2310.01039,Software Reconfiguration in Robotics,"Sven Peldszus, Davide Brugali, Daniel Struber, Patrizio Pelliccione, T. Berger",arXiv.org,2023,"Since it has often been claimed by academics that reconfiguration is essential, many approaches to reconfiguration, especially of robotic systems, have been developed. Accordingly, the literature on robotics is rich in techniques for reconfiguring robotic systems. However, when talking to researchers in the domain, there seems to be no common understanding of what exactly reconfiguration is and how it relates to other concepts such as adaptation. Beyond this academic perspective, robotics frameworks provide mechanisms for dynamically loading and unloading parts of robotics applications. While we have a fuzzy picture of the state-of-the-art in robotic reconfiguration from an academic perspective, we lack a picture of the state-of-practice from a practitioner perspective. To fill this gap, we survey the literature on reconfiguration in robotic systems by identifying and analyzing 98 relevant papers, review how four major robotics frameworks support reconfiguration, and finally investigate the realization of reconfiguration in 48 robotics applications. When comparing the state-of-the-art with the state-of-practice, we observed a significant discrepancy between them, in particular, the scientific community focuses on complex structural reconfiguration, while in practice only parameter reconfiguration is widely used. Based on our observations, we discuss possible reasons for this discrepancy and conclude with a takeaway message for academics and practitioners interested in robotics.",265297423,1,153,,
10.1145/3652620.3688340,Towards Rapid Design of Compartmental Models,"Zahra Fiyouzisabah, Jessie Galasso, Marios Fokaefs, Michalis Famelis",Proceedings of the ACM/IEEE 27th International Conference on Model Driven Engineering Languages and Systems,2024,"In times of crisis, epidemiologists can come under great pressure to model rapidly evolving diseases and to produce analyses about the effects of potential public health interventions. Taking previously developed, tested, and validated model components as the base on which to prototype new infectious disease models can save precious time and effort. However, there is currently no systematic process for quickly navigating a corpus of existing epidemiological models or identifying and reusing their most useful components. In this paper, we propose a vision to accelerate the creation of prototype compartmental models for infectious diseases. We outline a semi-automated process that epidemiologists can use to create pro-totypes that have been partially completed with reused fragments from existing models. Epidemiologists can thus focus on modelling the novel aspects of an ongoing public health crisis, as opposed to aspects of it that are already more or less well understood in previous work. Our approach comprises five steps in total, including identifying useful components in a corpus of infectious disease models, generating potential candidate prototypes, and organizing them in a formal data structure that allows navigation and exploration by the modellers. We outline 13 challenges ahead and discuss potential solutions based on formal modelling techniques.",280900066,0,35,,
10.1016/j.jss.2024.112152,Automated program repair for variability bugs in software product line systems,"Thu-Trang Nguyen, Zheng Zheng, Paolo Arcaini, Fuyuki Ishikawa, Hieu Dinh Vo",Journal of Systems and Software,2024,(missing abstract),277376600,1,105,,
10.1145/3611663,Finding Near-optimal Configurations in Colossal Spaces with Statistical Guarantees,"Jeho Oh, D. Batory, R. Heradio",ACM Transactions on Software Engineering and Methodology,2023,"A Software Product Line (SPL) is a family of similar programs. Each program is defined by a unique set of features, called a configuration, that satisfies all feature constraints. âWhat configuration achieves the best performance for a given workload?â is the SPLOptimization (SPLO) challenge. SPLO is daunting: just 80 unconstrained features yield 1024 unique configurations, which equals the estimated number of stars in the universe. We explain (a) how uniform random sampling and random search algorithms solve SPLO more efficiently and accurately than current machine-learned performance models and (b) how to compute statistical guarantees on the quality of a returned configuration; i.e., it is within x% of optimal with y% confidence.",264861245,7,111,,
10.1145/3522664.3528602,Black-Box Models for Non-Functional Properties of AI Software Systems,"Daniel Friesel, O. Spinczyk",2022 IEEE/ACM 1st International Conference on AI Engineering â Software Engineering for AI (CAIN),2022,"Non-functional properties (NFPs) such as latency, memory requirements, or hardware cost are an important characteristic of AI software systems, especially in the domain of resource-constrained embedded devices. Embedded AI products require sufficient resources for satisfactory latency and accuracy, but should also be cost-efficient and therefore not use more powerful hardware than strictly necessary. Traditionally, modeling and optimization efforts focus on the AI architecture, utilizing methods such as neural architecture search (NAS). However, before developers can start optimizing, they need to know which architectures are suitable candidates for their use case. To this end, architectures must be viewed in context: model post-processing (e.g. quantization), hardware platform, and run-time configuration such as batching all have significant effects on NFPs and therefore on AI architecture performance. Moreover, scalar parameters such as batch size cannot be benchmarked exhaustively. We argue that it is worthwhile to address this issue by means of black-box models before deciding on AI architectures for optimization and hardware/software platforms for inference. To support our claim, we present an AI product line with variable hardware and software components, perform benchmarks, and present notable results. Additionally, we evaluate both compactness and generalization capabilities of regression tree-based modeling approaches from the machine learning and product line engineering communities. We find that linear model trees perform best: they can capture NFPs of known AI configurations with a mean error of up to 13 %, and can predict unseen configurations with a mean error of 10 to 26 %. We find linear model trees to be more compact and interpretable than other tree-based approaches. CCS CONCEPTS â¢ Computing methodologies â Modeling methodologies; Machine learning; â¢ Software and its engineering â Extra-functional properties.",247916766,4,27,,
10.1007/978-3-031-08129-3_3,Scratching the Surface of ./configure: Learning the Effects of Compile-Time Options on Binary Size and Gadgets,"Xhevahire TÃ«rnava, Mathieu Acher, Luc Lesoil, Arnaud Blouin, Jean-Marc JÃ©zÃ©quel",Springer eBooks,2022,"AbstractNumerous software systems are configurable through compile-time options and the widely used ./configure. However, the combined effects of these options on binaryâs non-functional properties (size and attack surface) are often not documented, and or not well understood, even by experts. Our goal is to provide automated support for exploring and comprehending the configuration space (a.k.a., surface) of compile-time options using statistical learning techniques. In this paper, we perform an empirical study on four C-based configurable systems. We measure the variation of binary size and attack surface (by quantifying the number of code reuse gadgets) in over 400 compile-time configurations of a subject system. We then apply statistical learning techniques on top of our build infrastructure to identify how compile-time options relate to non-functional properties. Our results show that, by changing the default configuration, the systemâs binary size and gadgets vary greatly (roughly \(-79\%\) to \(244\%\) and \(-77\%\) to \(30\%\), respectively). Then, we found out that identifying the most influential options can be accurately learned with a small training set, while their relative importance varies across size and attack surface for the same system. Practitioners can use our approach and artifacts to explore the effects of compile-time options in order to take informed decisions when configuring a system with ./configure. KeywordsConfigurable systemsCompile-time variabilityBinary sizeGadgetsSystem securityNon-functional propertiesStatistical learning",234291986,0,27,,
